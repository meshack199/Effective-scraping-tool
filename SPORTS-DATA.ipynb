{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THIS IS A CHALLENGE TO SCRAPE DATA FROM SPORTYBET APP\n",
    "###### objectives\n",
    "- extract specific data i.e \n",
    "- export the dataset in csv format to local storage.\n",
    "- create a bot to automatically check for new entrie every 24hrs\n",
    "- next project: make sense of the dataset using ds and ml techniques\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE BREAKDOWN\n",
    "\n",
    "###### DATA EXTRACTION\n",
    "- we want to scrape all data related to football\n",
    "- This includes; matches, odds, their related league, date and betting markets with their respective oddds.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1. Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import  Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Invoking requests to access the website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_params = {\n",
    "      'api_key': 'your scrape-ops API KEY',\n",
    "      'url': 'https://www.sportybet.com/ke/sport/football/upcoming?time=0', \n",
    "      'render_js': True,\n",
    "  }\n",
    "response = requests.get(\n",
    "  url='https://proxy.scrapeops.io/v1/', \n",
    "  params=urlencode(proxy_params),\n",
    "  timeout=120,\n",
    ").text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPORTS=[]\n",
    "soup = BeautifulSoup(response, 'lxml')\n",
    "data=soup.find('div',class_='import-match')\n",
    "dataset=data.find_all('div', class_=\"match-league-wrap\")\n",
    "for match_details in dataset:\n",
    "    Match_date=match_details.find('div', class_='m-table-cell date').text\n",
    "    game_ID=match_details.find('div',class_='game-id').text\n",
    "    Match_time=match_details.find('div',class_='clock-time').text\n",
    "    Match_time=Match_time[:-2]\n",
    "    Home_team=match_details.find('div',class_='home-team').text\n",
    "    Away_team=match_details.find('div', class_='away-team').text\n",
    "    Three_way_odds=match_details.find('div', class_='m-market market').text\n",
    "    SPORTS.append([Match_date,game_ID,Match_time, Home_team,Away_team,Three_way_odds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.DataFrame(SPORTS,columns=['Match_date','game_ID','Match_time','Home_team','Away_team','Three_way_odds'])\n",
    "df.to_csv(\"sportsy_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "430b663cd334eddbd1df347f4a4848e2a41c8f1459b949cdd62a307a5910a359"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
